{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76009778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f3f7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128 0.23.0+cu128\n",
      "cuda available: True\n",
      "cudnn version: 91002\n",
      "Device name: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__, torchvision.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cudnn version:\", torch.backends.cudnn.version())\n",
    "# Get device name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a25e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "#device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf1c2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, d_in = 4, d_hidden = 64):\n",
    "        super().__init__()\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_in, d_hidden), torch.nn.SiLU(),\n",
    "            torch.nn.Linear(d_hidden, d_hidden), torch.nn.SiLU(),\n",
    "            torch.nn.Linear(d_hidden, 1)\n",
    "        )\n",
    "    def forward(self, x):  # x: [B,4] -> [B,1]\n",
    "        return self.mlp(x)\n",
    "\n",
    "net = Net().to(device)\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f4f4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor(100.0, device=device)\n",
    "\n",
    "def one_path_label_and_grads(x, antithetic=True):\n",
    "    \"\"\"\n",
    "    x: [B,4] with grads enabled on inputs.\n",
    "    Returns: y (price) [B,1], g (dy/dx) [B,4]\n",
    "    \"\"\"\n",
    "    B = x.shape[0]\n",
    "    S0, vol, r, T = [x[:,i:i+1] for i in range(4)]\n",
    "    # One Normal per sample (+ antithetic for variance reduction)\n",
    "    Z = torch.randn(B, 1, device=device)\n",
    "    if antithetic:\n",
    "        Z = torch.cat([Z, -Z], dim=1)  # [B,2]\n",
    "    # Expand params to both paths\n",
    "    Texp = T\n",
    "    drift = (r - 0.5*vol**2) * Texp\n",
    "    diff  = vol * torch.sqrt(Texp + 1e-12)\n",
    "    ST = S0 * torch.exp(drift + diff * Z)           # [B,2]\n",
    "    payoff = torch.clamp(ST - K, min=0.0)           # [B,2]\n",
    "    disc = torch.exp(-r * Texp)                     # [B,1]\n",
    "    y = disc * payoff.mean(dim=1, keepdim=True)     # [B,1]\n",
    "    # Pathwise derivatives via autograd\n",
    "    grads = torch.autograd.grad(\n",
    "        y.sum(), (S0, vol, r, T), create_graph=False, retain_graph=False\n",
    "    )\n",
    "    g = torch.cat([gi for gi in grads], dim=1)      # [B,4]\n",
    "    return y.detach(), g.detach()\n",
    "\n",
    "# --- Training loop: online sampling, single path per sample ---\n",
    "def sample_batch(B):\n",
    "    # Choose your training distribution for states (very important!)\n",
    "    S0 = torch.exp(torch.randn(B)*0.25 + math.log(100.0))\n",
    "    vol = torch.clamp(torch.randn(B)*0.05 + 0.2, 0.05, 0.6)\n",
    "    r   = torch.clamp(torch.randn(B)*0.01 + 0.01, -0.01, 0.05)\n",
    "    T   = torch.clamp(torch.rand(B) * 2.0, 0.05, 2.0)\n",
    "    x = torch.stack([S0, vol, r, T], dim=1).to(device).requires_grad_(True)\n",
    "    return x\n",
    "\n",
    "def featurize(x, K):\n",
    "    # x: [B,4] = (S0, vol, r, T)\n",
    "    S0, vol, r, T = [x[:,i:i+1] for i in range(4)]\n",
    "    m   = torch.log(S0 / K)         # log-moneyness\n",
    "    v   = torch.log(vol)            # log-vol\n",
    "    tau = torch.sqrt(T + 1e-12)     # time feature\n",
    "    z = torch.cat([m, v, r, tau], dim=1)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c26c0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██| 5000/5000 [00:35<00:00, 139.43it/s, beta=1.00, grad=10.99, loss=55.44, val=44.45]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- tiny helper: per-batch scale for gradient loss (robust to outliers) ---\n",
    "def robust_scale(g, eps=1e-8):\n",
    "    # median absolute deviation-like scale per coordinate\n",
    "    med = g.median(dim=0).values\n",
    "    mad = (g - med).abs().median(dim=0).values + eps\n",
    "    # Prevent too-small scales\n",
    "    return torch.clamp(mad, min=1e-3)\n",
    "\n",
    "# --- model ---\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, d_in=4, d_hidden=32):\n",
    "        super().__init__()\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_in, d_hidden), torch.nn.SiLU(),\n",
    "            torch.nn.Linear(d_hidden, d_hidden), torch.nn.SiLU(),\n",
    "            torch.nn.Linear(d_hidden, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "net = Net().to(device)\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=3e-4, weight_decay=0.0)\n",
    "\n",
    "# --- training ---\n",
    "steps = 5000\n",
    "B = 3000\n",
    "alpha = 1.0\n",
    "\n",
    "beta_start, beta_end = 5.0, 1.0      # start gradient-heavy, then ease\n",
    "warmup_steps = int(0.6 * steps)      # linear schedule for beta\n",
    "\n",
    "pbar = tqdm(range(steps), desc=\"Training\", ncols=100)\n",
    "for step in pbar:\n",
    "    # 1) sample a batch of raw states\n",
    "    x = sample_batch(B)                              # [B,4]; requires_grad set inside sample_batch\n",
    "    y, g = one_path_label_and_grads(x)               # labels wrt RAW x (both detached inside function)\n",
    "\n",
    "    # 2) forward pass on a fresh, grad-enabled copy of x\n",
    "    x_ = x.detach().clone().requires_grad_(True)\n",
    "    yhat = net(x_)\n",
    "\n",
    "    # 3) model gradient wrt x (to match simulator pathwise grads)\n",
    "    grads_pred = torch.autograd.grad(yhat.sum(), x_, create_graph=True)[0]\n",
    "\n",
    "    # 4) simple, robust scaling for gradient loss (per coordinate)\n",
    "    s = robust_scale(g).unsqueeze(0)                 # [1,4]\n",
    "    grad_res = (grads_pred - g) / s\n",
    "    grad_loss = (grad_res**2).mean()\n",
    "\n",
    "    # 5) price loss (squared error)\n",
    "    val_loss = ((yhat - y)**2).mean()\n",
    "\n",
    "    # 6) beta schedule\n",
    "    if step < warmup_steps:\n",
    "        t = step / max(1, warmup_steps)\n",
    "        beta = beta_start * (1 - t) + beta_end * t\n",
    "    else:\n",
    "        beta = beta_end\n",
    "\n",
    "    loss = alpha*val_loss + beta*grad_loss\n",
    "\n",
    "    # 7) step\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "    opt.step()\n",
    "\n",
    "    # 8) pretty progress\n",
    "    if (step % 50) == 0:\n",
    "        pbar.set_postfix(\n",
    "            loss=f\"{loss.item():.2f}\",\n",
    "            val=f\"{val_loss.item():.2f}\",\n",
    "            grad=f\"{grad_loss.item():.2f}\",\n",
    "            beta=f\"{beta:.2f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f43b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualBlock(torch.nn.Module):\n",
    "#     def __init__(self, d, width):\n",
    "#         super().__init__()\n",
    "#         self.lin1 = torch.nn.Linear(d, width)\n",
    "#         self.lin2 = torch.nn.Linear(width, d)\n",
    "#         self.act  = torch.nn.SiLU()\n",
    "#     def forward(self, x):\n",
    "#         h = self.act(self.lin1(x))\n",
    "#         h = self.lin2(h)\n",
    "#         return x + h\n",
    "\n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, d_in=4, width=64, nblocks=3):\n",
    "#         super().__init__()\n",
    "#         self.inp = torch.nn.Linear(d_in, width)\n",
    "#         self.blocks = torch.nn.ModuleList([ResidualBlock(width, width) for _ in range(nblocks)])\n",
    "#         self.out = torch.nn.Linear(width, 1)\n",
    "#         self.act = torch.nn.SiLU()\n",
    "#     def forward(self, z):          # z are features from featurize(...)\n",
    "#         h = self.act(self.inp(z))\n",
    "#         for b in self.blocks:\n",
    "#             h = self.act(b(h))\n",
    "#         return self.out(h)\n",
    "\n",
    "# net = Net().to(device)\n",
    "# opt = torch.optim.AdamW(net.parameters(), lr=3e-4, weight_decay=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850349f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# y doesn't depend on z_for_label directly, but on x_; we need ∂y/∂z = (∂y/∂x) * (∂x/∂z)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Instead of manual chain-rule, get ∂y/∂z by differentiating y wrt z_for_label via implicit graph:\u001b[39;00m\n\u001b[32m     31\u001b[39m y_for_grad, _ = one_path_label_and_grads(x_)      \u001b[38;5;66;03m# y(x_)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m gy_wrt_z = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_for_grad\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_for_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m].detach()\n\u001b[32m     34\u001b[39m val_loss  = ((yhat - y)**\u001b[32m2\u001b[39m).mean()\n\u001b[32m     35\u001b[39m grad_loss = ((grads_pred_z - gy_wrt_z)**\u001b[32m2\u001b[39m).mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CodeProjects\\diff_ml\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:503\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    499\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    500\u001b[39m         grad_outputs_\n\u001b[32m    501\u001b[39m     )\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    514\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    515\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    516\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CodeProjects\\diff_ml\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# K = torch.tensor(100.0, device=device)\n",
    "\n",
    "# alpha, beta = 1.0, 5.0\n",
    "# for step in range(5000):\n",
    "#     B = 512\n",
    "#     x = sample_batch(B)                     # your sampler; requires_grad already set inside\n",
    "#     y, g_x = one_path_label_and_grads(x)    # labels + pathwise grads wrt RAW x\n",
    "\n",
    "#     # --- features ---\n",
    "#     z = featurize(x, K)                     # [B,4]  (no detach; we only need grads wrt z for the model)\n",
    "\n",
    "#     # --- forward ---\n",
    "#     z_ = z.detach().clone().requires_grad_(True)\n",
    "#     yhat = net(z_)\n",
    "\n",
    "#     # --- model Jacobian wrt features z ---\n",
    "#     grads_pred_z = torch.autograd.grad(yhat.sum(), z_, create_graph=True)[0]\n",
    "\n",
    "#     # --- loss: value + gradient match ---\n",
    "#     # NOTE: g_x are grads wrt RAW x. Because the model is f(z(x)), matching grads wrt z is fine:\n",
    "#     #       we’re asking the net to learn d/dz of f, which is coherent as long as we always feed z.\n",
    "#     #       (You could also transform g_x to z-space via chain rule, but here we keep it simple:\n",
    "#     #        train the model as a function of z and match its ∂/∂z to simulator ∂y/∂z computed via autograd through featurize.)\n",
    "#     # Compute simulator grads wrt z using autograd through featurize:\n",
    "#     #   z = φ(x), y = y(x)        -> ∂y/∂z = (∂y/∂x) * (∂x/∂z)\n",
    "#     # It’s a bit cleaner to recompute ∂y/∂z directly:\n",
    "#     x_ = x.detach().clone().requires_grad_(True)\n",
    "#     z_for_label = featurize(x_, K)\n",
    "#     # y doesn't depend on z_for_label directly, but on x_; we need ∂y/∂z = (∂y/∂x) * (∂x/∂z)\n",
    "#     # Instead of manual chain-rule, get ∂y/∂z by differentiating y wrt z_for_label via implicit graph:\n",
    "#     y_for_grad, _ = one_path_label_and_grads(x_)      # y(x_)\n",
    "#     gy_wrt_z = torch.autograd.grad(y_for_grad.sum(), z_for_label, retain_graph=False, create_graph=False)[0].detach()\n",
    "\n",
    "#     val_loss  = ((yhat - y)**2).mean()\n",
    "#     grad_loss = ((grads_pred_z - gy_wrt_z)**2).mean()\n",
    "#     loss = alpha*val_loss + beta*grad_loss\n",
    "\n",
    "#     opt.zero_grad()\n",
    "#     loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "#     opt.step()\n",
    "\n",
    "#     if step % 500 == 0:\n",
    "#         print(step, f\"loss={float(loss):.4f}  val={float(val_loss):.4f}  grad={float(grad_loss):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "78ff4b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4259.559045539481\n",
      "200 4177.255755677336\n",
      "400 3327.7115667896674\n",
      "600 2611.4894760969546\n",
      "800 2624.1089982337803\n",
      "1000 3184.893391791928\n",
      "1200 2600.5811784652283\n",
      "1400 3264.611954987147\n",
      "1600 4982.693417683873\n",
      "1800 2846.061057497064\n",
      "2000 4042.258212188225\n",
      "2200 2897.1457562536843\n",
      "2400 3262.3354711007596\n",
      "2600 4932.153302748594\n",
      "2800 3387.4655739048667\n",
      "3000 3726.4271660410222\n",
      "3200 5310.121768827692\n",
      "3400 3659.1867767549656\n",
      "3600 4583.559063653327\n",
      "3800 2298.277520647318\n",
      "4000 3670.0058195820384\n",
      "4200 1745.3328547701385\n",
      "4400 4429.387784103051\n",
      "4600 3093.0967924813913\n",
      "4800 3319.23684096691\n",
      "5000 2553.6353547932945\n",
      "5200 5756.225714453664\n",
      "5400 4182.995632913563\n",
      "5600 3868.662184963603\n",
      "5800 3020.6810771064115\n",
      "6000 2011.8831412864652\n",
      "6200 2931.321377236097\n",
      "6400 3589.226707404174\n",
      "6600 2547.6018627409503\n",
      "6800 1924.7501741159394\n",
      "7000 3795.262267549007\n",
      "7200 2998.3106842399984\n",
      "7400 2026.5782633762733\n",
      "7600 3143.482910427123\n",
      "7800 3991.9112592545052\n",
      "8000 3677.3876425949743\n",
      "8200 4763.023528997892\n",
      "8400 2142.891076402088\n",
      "8600 3106.421693642811\n",
      "8800 5256.8453723961\n",
      "9000 2163.7160646267034\n",
      "9200 3205.2777502910717\n",
      "9400 4748.741483128548\n",
      "9600 4886.509098998333\n",
      "9800 2981.9874046410137\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = 1.0, 1.0         # balance value vs gradient fitting\n",
    "for step in range(10000):\n",
    "    B = 100\n",
    "    x = sample_batch(B)\n",
    "    y, g = one_path_label_and_grads(x, False)\n",
    "    # Forward + model gradients\n",
    "    x_ = x.detach().clone().requires_grad_(True)\n",
    "    yhat = net(x_)\n",
    "    # ∇_x f(x) via autograd\n",
    "    grads_pred = torch.autograd.grad(\n",
    "        yhat.sum(), x_, create_graph=False, retain_graph=True\n",
    "    )[0]\n",
    "    # Sobolev loss\n",
    "    loss = alpha*((yhat - y)**2).mean() + beta*((grads_pred - g)**2).mean()\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "    if step % 200 == 0:\n",
    "        print(step, float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88bfc6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 | loss 2320.607  val 1436.659  grad 176.790  beta 5.00\n",
      " 200 | loss 28.551  val 26.162  grad 0.508  beta 4.70\n",
      " 400 | loss 16.766  val 15.631  grad 0.258  beta 4.40\n",
      " 600 | loss 14.467  val 13.221  grad 0.304  beta 4.10\n",
      " 800 | loss 13.258  val 11.754  grad 0.396  beta 3.80\n",
      "1000 | loss 15.332  val 13.015  grad 0.662  beta 3.50\n",
      "1200 | loss 16.900  val 13.233  grad 1.146  beta 3.20\n",
      "1400 | loss 19.008  val 14.852  grad 1.433  beta 2.90\n",
      "1600 | loss 17.131  val 14.438  grad 1.036  beta 2.60\n",
      "1800 | loss 15.008  val 12.872  grad 0.929  beta 2.30\n",
      "2000 | loss 16.986  val 14.950  grad 1.018  beta 2.00\n",
      "2200 | loss 12.878  val 11.421  grad 0.857  beta 1.70\n",
      "2400 | loss 15.321  val 13.571  grad 1.251  beta 1.40\n",
      "2600 | loss 13.299  val 12.432  grad 0.789  beta 1.10\n",
      "2800 | loss 13.820  val 13.037  grad 0.979  beta 0.80\n",
      "3000 | loss 11.024  val 10.651  grad 0.745  beta 0.50\n",
      "3200 | loss 14.911  val 14.394  grad 1.035  beta 0.50\n",
      "3400 | loss 12.989  val 12.472  grad 1.034  beta 0.50\n",
      "3600 | loss 10.239  val 9.859  grad 0.761  beta 0.50\n",
      "3800 | loss 11.115  val 10.778  grad 0.674  beta 0.50\n",
      "4000 | loss 12.341  val 11.914  grad 0.854  beta 0.50\n",
      "4200 | loss 10.742  val 10.405  grad 0.673  beta 0.50\n",
      "4400 | loss 12.400  val 11.984  grad 0.831  beta 0.50\n",
      "4600 | loss 9.894  val 9.572  grad 0.645  beta 0.50\n",
      "4800 | loss 11.098  val 10.769  grad 0.657  beta 0.50\n"
     ]
    }
   ],
   "source": [
    "import torch, math\n",
    "\n",
    "# --------- running scalers ---------\n",
    "# Standardize inputs using running mean/var (EMA)\n",
    "class RunningStandardizer:\n",
    "    def __init__(self, d, eps=1e-8, decay=0.01, device=\"cpu\"):\n",
    "        self.mu = torch.zeros(d, device=device)\n",
    "        self.nu = torch.ones(d, device=device)      # second moment\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, x):  # x: [B,d]\n",
    "        with torch.no_grad():\n",
    "            self.t += 1\n",
    "            m = x.mean(dim=0)\n",
    "            v = (x**2).mean(dim=0)\n",
    "            self.mu = (1 - self.decay)*self.mu + self.decay*m\n",
    "            self.nu = (1 - self.decay)*self.nu + self.decay*v\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return torch.sqrt(torch.clamp(self.nu - self.mu**2, min=self.eps))\n",
    "\n",
    "    def transform(self, x):\n",
    "        return (x - self.mu) / self.std\n",
    "\n",
    "    def inv_transform(self, z):\n",
    "        return z * self.std + self.mu\n",
    "\n",
    "# Diagonal whitening for gradient residuals: weight by 1/var\n",
    "class GradWhitening:\n",
    "    def __init__(self, d, eps=1e-8, decay=0.01, device=\"cpu\"):\n",
    "        self.m2 = torch.ones(d, device=device)  # E[g^2]\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "    def update(self, g):      # g: [B,d]\n",
    "        with torch.no_grad():\n",
    "            self.m2 = (1 - self.decay)*self.m2 + self.decay*(g**2).mean(dim=0)\n",
    "    @property\n",
    "    def inv_var(self):\n",
    "        return 1.0 / torch.clamp(self.m2, min=self.eps)\n",
    "\n",
    "device = next(net.parameters()).device\n",
    "x_scaler = RunningStandardizer(d=4, device=device)\n",
    "gw = GradWhitening(d=4, device=device)\n",
    "\n",
    "huber_delta = 5.0\n",
    "def huber(res, delta=huber_delta):\n",
    "    absr = torch.abs(res)\n",
    "    quad = torch.minimum(absr, torch.tensor(delta, device=res.device))\n",
    "    lin  = absr - quad\n",
    "    return 0.5*quad**2 + delta*lin\n",
    "\n",
    "alpha, beta0, beta1 = 1.0, 5.0, 0.5  # start gradient-heavy, then anneal down\n",
    "total_steps = 5000\n",
    "lr = 3e-4\n",
    "for g in opt.param_groups: g['lr'] = lr\n",
    "\n",
    "for step in range(total_steps):\n",
    "    B = 512\n",
    "    x_raw = sample_batch(B)                    # [B,4], raw\n",
    "    x_scaler.update(x_raw)\n",
    "    xz = x_scaler.transform(x_raw.detach())    # standardized inputs\n",
    "\n",
    "    # Labels and pathwise grads w.r.t. RAW x\n",
    "    y, g_raw = one_path_label_and_grads(x_raw) # detached\n",
    "\n",
    "    # Convert label-gradients to standardized coordinates:\n",
    "    # z = (x - mu)/std  =>  ∂/∂z = std * ∂/∂x   (elementwise)\n",
    "    std = x_scaler.std.unsqueeze(0)            # [1,4]\n",
    "    g_z = g_raw * std                          # chain rule\n",
    "\n",
    "    # Update whitening on g_z\n",
    "    gw.update(g_z)\n",
    "\n",
    "    # Model forward on standardized inputs\n",
    "    xz_ = xz.detach().clone().requires_grad_(True)\n",
    "    yhat = net(xz_)\n",
    "\n",
    "    # Model gradient wrt standardized inputs\n",
    "    grads_pred = torch.autograd.grad(yhat.sum(), xz_, create_graph=True)[0]\n",
    "\n",
    "    # Loss pieces\n",
    "    beta = beta0 * (1 - step/ (0.6*total_steps)) + beta1 * (step/(0.6*total_steps))\n",
    "    beta = float(max(beta, beta1))\n",
    "\n",
    "    # Huber on value residual (robust to one-path noise)\n",
    "    val_res = (yhat - y)\n",
    "    val_loss = huber(val_res).mean()\n",
    "\n",
    "    # Whitened gradient loss: sum_j w_j * (Δg_j)^2\n",
    "    w = gw.inv_var.unsqueeze(0)                 # [1,4]\n",
    "    grad_res = grads_pred - g_z\n",
    "    grad_loss = (w * (grad_res**2)).mean()\n",
    "\n",
    "    loss = alpha*val_loss + beta*grad_loss\n",
    "\n",
    "    opt.zero_grad()\n",
    "    torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        with torch.no_grad():\n",
    "            # Log per-term to see who dominates\n",
    "            print(f\"{step:4d} | loss {loss.item():.3f}  val {val_loss.item():.3f}  grad {grad_loss.item():.3f}  beta {beta:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
